{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {
    "vscode": {
     "languageId": "raw"
    }
   },
   "source": [
    "# Kaggle è³‡æ–™é›†ä¸‹è¼‰è…³æœ¬\n",
    "\n",
    "æ­¤è…³æœ¬ä½¿ç”¨ KaggleHub ä¸‹è¼‰è³‡æ–™é›†ï¼Œä¸¦å°‡æª”æ¡ˆè¤‡è£½åˆ°æŒ‡å®šçš„å°ˆæ¡ˆç›®éŒ„ä¸­ã€‚\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/yasserh/titanic-dataset?dataset_version_number=1...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 22.0k/22.0k [00:00<00:00, 1.65MB/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n",
      "Path to dataset files: /home/os-sunnie.gd.weng/.cache/kagglehub/datasets/yasserh/titanic-dataset/versions/1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "import shutil\n",
    "from pathlib import Path\n",
    "\n",
    "def download_to_folder(dataset_id, target_folder, dataset_name=None):\n",
    "    \"\"\"\n",
    "    ä¸‹è¼‰ Kaggle è³‡æ–™é›†åˆ°æŒ‡å®šè³‡æ–™å¤¾\n",
    "    \n",
    "    Args:\n",
    "        dataset_id: Kaggle è³‡æ–™é›† ID (å¦‚ 'yasserh/titanic-dataset')\n",
    "        target_folder: ç›®æ¨™è³‡æ–™å¤¾è·¯å¾‘\n",
    "        dataset_name: è³‡æ–™é›†åç¨±ï¼ˆç”¨æ–¼é¡¯ç¤ºï¼‰\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # ä½¿ç”¨ KaggleHub ä¸‹è¼‰ï¼ˆæœƒä¸‹è¼‰åˆ°ç·©å­˜ç›®éŒ„ï¼‰\n",
    "        print(f\"ðŸ“¥ æ­£åœ¨ä¸‹è¼‰ {dataset_name or dataset_id}...\")\n",
    "        cache_path = kagglehub.dataset_download(dataset_id)\n",
    "        print(f\"âœ… ä¸‹è¼‰å®Œæˆï¼Œç·©å­˜è·¯å¾‘: {cache_path}\")\n",
    "        \n",
    "        # å‰µå»ºç›®æ¨™è³‡æ–™å¤¾\n",
    "        target_path = Path(target_folder)\n",
    "        target_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # è¤‡è£½æª”æ¡ˆåˆ°ç›®æ¨™è³‡æ–™å¤¾\n",
    "        print(f\"ðŸ“ è¤‡è£½æª”æ¡ˆåˆ°: {target_folder}\")\n",
    "        \n",
    "        # å¦‚æžœç›®æ¨™è³‡æ–™å¤¾å·²å­˜åœ¨ä¸”ä¸ç‚ºç©ºï¼Œå…ˆæ¸…ç©º\n",
    "        if target_path.exists() and any(target_path.iterdir()):\n",
    "            shutil.rmtree(target_path)\n",
    "            target_path.mkdir(parents=True, exist_ok=True)\n",
    "        \n",
    "        # è¤‡è£½æ‰€æœ‰æª”æ¡ˆ\n",
    "        cache_path_obj = Path(cache_path)\n",
    "        if cache_path_obj.is_file():\n",
    "            # å¦‚æžœæ˜¯å–®å€‹æª”æ¡ˆ\n",
    "            shutil.copy2(cache_path, target_path / cache_path_obj.name)\n",
    "        else:\n",
    "            # å¦‚æžœæ˜¯è³‡æ–™å¤¾ï¼Œè¤‡è£½æ‰€æœ‰å…§å®¹\n",
    "            for item in cache_path_obj.iterdir():\n",
    "                if item.is_file():\n",
    "                    shutil.copy2(item, target_path / item.name)\n",
    "                elif item.is_dir():\n",
    "                    shutil.copytree(item, target_path / item.name)\n",
    "        \n",
    "        print(f\"âœ… è¤‡è£½å®Œæˆï¼\")\n",
    "        \n",
    "        # é¡¯ç¤ºä¸‹è¼‰çš„æª”æ¡ˆ\n",
    "        files = list(target_path.glob('*'))\n",
    "        print(f\"ðŸ“„ è³‡æ–™å¤¾ä¸­çš„æª”æ¡ˆ: {[f.name for f in files]}\")\n",
    "        \n",
    "        return str(target_path)\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ ä¸‹è¼‰ {dataset_name or dataset_id} å¤±æ•—: {e}\")\n",
    "        return None\n",
    "\n",
    "# è¨­ç½®åŸºæœ¬ç›®éŒ„\n",
    "base_dir = \"datasets/raw\"\n",
    "os.makedirs(base_dir, exist_ok=True)\n",
    "\n",
    "print(\"ðŸš€ é–‹å§‹ä¸‹è¼‰ Kaggle è³‡æ–™é›†åˆ°æŒ‡å®šè³‡æ–™å¤¾...\")\n",
    "print(\"=\" * 50)\n",
    "\n",
    "# ä¸‹è¼‰ Titanic è³‡æ–™é›†\n",
    "titanic_path = download_to_folder(\n",
    "    dataset_id=\"yasserh/titanic-dataset\",\n",
    "    target_folder=f\"{base_dir}/titanic\",\n",
    "    dataset_name=\"Titanic Dataset\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ä¸‹è¼‰å…¶ä»–è³‡æ–™é›†\n",
    "datasets = [\n",
    "    {\n",
    "        \"id\": \"mirichoi0218/insurance\",\n",
    "        \"name\": \"Medical Cost Personal Dataset\",\n",
    "        \"folder\": \"insurance\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"uciml/breast-cancer-wisconsin-data\",\n",
    "        \"name\": \"Breast Cancer Wisconsin\",\n",
    "        \"folder\": \"breast_cancer\"\n",
    "    },\n",
    "    {\n",
    "        \"id\": \"lakshmi25npathi/imdb-dataset-of-50k-movie-reviews\",\n",
    "        \"name\": \"IMDB 50K Movie Reviews\",\n",
    "        \"folder\": \"imdb_reviews\"\n",
    "    }\n",
    "]\n",
    "\n",
    "successful_downloads = []\n",
    "failed_downloads = []\n",
    "\n",
    "for dataset in datasets:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    result = download_to_folder(\n",
    "        dataset_id=dataset[\"id\"],\n",
    "        target_folder=f\"{base_dir}/{dataset['folder']}\",\n",
    "        dataset_name=dataset[\"name\"]\n",
    "    )\n",
    "    \n",
    "    if result:\n",
    "        successful_downloads.append(dataset[\"name\"])\n",
    "    else:\n",
    "        failed_downloads.append(dataset[\"name\"])\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(\"ðŸ“Š ä¸‹è¼‰çµæžœç¸½çµ:\")\n",
    "print(f\"âœ… æˆåŠŸä¸‹è¼‰ ({len(successful_downloads)} å€‹):\")\n",
    "for name in successful_downloads:\n",
    "    print(f\"   â€¢ {name}\")\n",
    "\n",
    "if failed_downloads:\n",
    "    print(f\"\\nâŒ ä¸‹è¼‰å¤±æ•— ({len(failed_downloads)} å€‹):\")\n",
    "    for name in failed_downloads:\n",
    "        print(f\"   â€¢ {name}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# æª¢æŸ¥æœ€çµ‚çš„è³‡æ–™å¤¾çµæ§‹\n",
    "print(\"\\nðŸ“ æœ€çµ‚è³‡æ–™å¤¾çµæ§‹:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "import os\n",
    "for root, dirs, files in os.walk(base_dir):\n",
    "    level = root.replace(base_dir, '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f\"{indent}{os.path.basename(root)}/\")\n",
    "    subindent = ' ' * 2 * (level + 1)\n",
    "    for file in files:\n",
    "        print(f\"{subindent}{file}\")\n",
    "\n",
    "# é¡¯ç¤ºæ¯å€‹è³‡æ–™é›†çš„è©³ç´°ä¿¡æ¯\n",
    "print(f\"\\nðŸ“Š å„è³‡æ–™é›†è©³ç´°è³‡è¨Š:\")\n",
    "print(\"=\" * 40)\n",
    "\n",
    "for folder in os.listdir(base_dir):\n",
    "    folder_path = os.path.join(base_dir, folder)\n",
    "    if os.path.isdir(folder_path):\n",
    "        files = os.listdir(folder_path)\n",
    "        file_count = len(files)\n",
    "        total_size = sum(os.path.getsize(os.path.join(folder_path, f)) \n",
    "                        for f in files if os.path.isfile(os.path.join(folder_path, f)))\n",
    "        size_mb = total_size / (1024 * 1024)\n",
    "        \n",
    "        print(f\"\\nðŸ“‚ {folder}:\")\n",
    "        print(f\"   â€¢ æª”æ¡ˆæ•¸é‡: {file_count}\")\n",
    "        print(f\"   â€¢ ç¸½å¤§å°: {size_mb:.2f} MB\")\n",
    "        print(f\"   â€¢ æª”æ¡ˆåˆ—è¡¨: {', '.join(files)}\")\n",
    "\n",
    "print(f\"\\nðŸŽ‰ è³‡æ–™é›†ä¸‹è¼‰å®Œæˆï¼æ‰€æœ‰æª”æ¡ˆå·²å­˜æ”¾åœ¨ '{base_dir}' è³‡æ–™å¤¾ä¸­ã€‚\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
