# 模組七講義：特徵選擇與降維 (Feature Selection and Dimensionality Reduction)

---

## 1. 本章學習框架 (Learning Framework)

- **第一原理 (First Principle)**: **「少即是多 (Less is More)」。** 在機器學習中，並非所有特徵都是平等的。加入過多無關或冗餘的特徵會導致模型性能下降、訓練時間變長、解釋性變差，這種現象被稱為「維度災難」(Curse of Dimensionality)。有效的特徵選擇與降維是建立高效、穩健模型的基石。

- **基礎 (Fundamentals)**: 本章節旨在教授特徵選擇的三大主流方法與一種關鍵的降維技術：
  1.  **過濾法 (Filter Methods)**: 快速、獨立於模型的初步篩選。
  2.  **包裹法 (Wrapper Methods)**: 以模型表現為導向的特徵子集搜索。
  3.  **嵌入法 (Embedded Methods)**: 將特徵選擇融入模型訓練過程。
  4.  **降維 (Dimensionality Reduction)**: 轉換特徵空間，創造新的、更精簡的特徵。

---

## 2. 過濾法 (Filter Methods)

> *「根據特徵本身的統計屬性進行評分和篩選，完全不涉及任何機器學習模型。」*

- **核心思想**: 透過統計檢驗來衡量特徵與目標變數之間的關聯性，或者衡量特徵本身的變異性。這是一種預處理步驟。
- **實現方式**:
  - **移除低變異數特徵**: `VarianceThreshold` - 刪除那些在所有樣本中都幾乎不變的特徵（例如，一個值佔了99%的特徵）。
  - **單變量統計檢驗**: `SelectKBest` / `SelectPercentile`
    - **迴歸**: `f_regression` (F檢驗)，計算每個特徵與目標變數的線性關係。
    - **分類**: `chi2` (卡方檢驗，用於非負特徵)、`f_classif` (F檢驗)。
  - **相關係數法**: 計算特徵與特徵之間的相關矩陣 (`df.corr()`)，移除那些與其他特徵高度相關的冗餘特徵。
- **優缺點**:
  - **優點**: 計算速度快，不易過擬合，獨立於模型，可作為特徵工程的初步探索。
  - **缺點**: 忽略了特徵之間的組合效應，可能移除掉那些單獨看來無用但組合起來有用的特徵。

---

## 3. 包裹法 (Wrapper Methods)

> *「將特徵選擇過程『包裹』在一個特定模型周圍，使用模型的性能作為評估特徵子集好壞的標準。」*

- **核心思想**: 這是一個搜索問題。目標是找到能讓特定模型（如邏輯回歸、隨機森林）性能達到最佳的特徵子集。
- **實現流程**:
  1.  **遞迴特徵消除 (Recursive Feature Elimination, RFE)**:
      - **流程**: 用所有特徵訓練一個模型 -> 移除最不重要的特徵 -> 重複此過程，直到剩下指定數量的特徵。
      - **前提**: 需要一個能提供特徵重要性評分的模型（如 `coef_` 或 `feature_importances_`）。
  2.  **序列特徵選擇 (Sequential Feature Selection, SFS)**:
      - **向前選擇 (Forward)**: 從空集合開始，每次加入一個能最大化模型性能的特徵。
      - **向後選擇 (Backward)**: 從完整集合開始，每次移除一個對模型性能影響最小的特徵。
- **優缺點**:
  - **優點**: 直接針對特定模型的性能進行優化，考慮了特徵之間的交互作用，通常效果比過濾法好。
  - **缺點**: 計算成本極高，因為需要反覆訓練模型。在數據集很大時可能不可行，且有過擬合到特定模型的風險。

---

## 4. 嵌入法 (Embedded Methods)

> *「在模型訓練的過程中，自動完成特徵選擇。特徵選擇是模型學習演算法的一部分。」*

- **核心思想**: 某些模型本身就具備篩選特徵的能力，通常是透過對不重要的特徵施加懲罰來實現。
- **實現方式**:
  - **基於懲罰項的模型 (L1正規化)**:
    - **Lasso Regression**: L1正規化會將不重要特徵的係數（`coef_`）壓縮至**恰好為零**，從而自然地移除了這些特徵。
  - **基於樹的模型**:
    - **Random Forest / Gradient Boosting (LightGBM, XGBoost)**: 這些模型在建樹的過程中，會計算每個特徵的貢獻度（`feature_importances_`）。我們可以基於這個重要性分數來篩選特徵。
- **優缺點**:
  - **優點**: 計算效率高於包裹法，考慮了特徵交互，且通常效果良好。是三種方法中最常用、最受歡迎的一種。
  - **缺點**: 特徵選擇的結果與所使用的模型強烈綁定。

---

## 5. 降維 (Dimensionality Reduction)

> *「降維不是『選擇』特徵，而是『創造』新的特徵。它將高維數據投影到一個低維子空間中，生成原始特徵的線性或非線性組合。」*

- **核心思想**: 數據中最重要的資訊可能不體現在單一維度上，而是分佈在多個維度的組合中。降維旨在找到這些「主成分」。
- **與特徵選擇的區別**:
  - **特徵選擇**: 從 `[f1, f2, f3, f4]` 中選出 `[f1, f3]`。新特徵是原始特徵的子集，**可解釋性強**。
  - **降維**: 將 `[f1, f2, f3, f4]` 轉換為 `[c1, c2]`，其中 `c1 = a*f1 + b*f2 + c*f3 + d*f4`。新特徵是原始特徵的組合，**失去了原有的物理意義**。
- **主要方法**:
  - **主成分分析 (Principal Component Analysis, PCA)**: 一種線性降維技術，旨在找到數據中方差最大的方向（主成分），並將數據投影到這些方向上，且各主成分之間線性無關。
- **應用場景**: 當特徵數量極大（如圖像、基因數據）時，或當特徵之間存在高度共線性時，PCA是非常有效的預處理手段。

---

## 6. 總結

特徵選擇與降維是應對維度災難、提升模型性能和可解釋性的關鍵武器。

- **快速初篩**: 使用 **過濾法** 快速排除無關和冗餘的特徵。
- **效果導向**: 如果計算資源允許，使用 **包裹法** 為特定模型尋找最佳特徵組合。
- **高效實用**: 在大多數情況下，**嵌入法**（特別是基於樹模型的重要性）是在效率和效果之間取得最佳平衡的選擇。
- **高維壓縮**: 當面臨成千上萬的特徵時，考慮使用 **PCA** 進行降維，以捕捉數據的主要結構。

選擇哪種方法取決於你的數據、計算資源和最終目標，但通常會組合使用這些技術以達到最佳效果。 