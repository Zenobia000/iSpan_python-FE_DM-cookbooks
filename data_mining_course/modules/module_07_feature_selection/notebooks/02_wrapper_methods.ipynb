{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "049d9b59",
   "metadata": {},
   "source": [
    "# Module 7: 特徵選擇 - 2. 包裹法 (Wrapper Methods)\n",
    "\n",
    "## 學習目標\n",
    "- 理解包裹法原理：掌握包裹法如何透過模型效能來選擇特徵。\n",
    "- 學習前向特徵選擇 (SFS)：實作並理解其從無到有逐步添加特徵的機制。\n",
    "- 學習後向特徵消除 (BFE)：實作並理解其從全集逐步移除特徵的機制。\n",
    "- 掌握遞歸特徵消除 (RFE/RFECV)：實作並理解其基於特徵重要性迭代篩選的機制。\n",
    "- 比較不同包裹法的優缺點：了解各方法的計算成本、特徵交互處理能力及應用場景。\n",
    "\n",
    "## 導論：模型如何「親自」參與特徵選擇？\n",
    "\n",
    "您的指南提到：「*包裹法會將特徵選擇問題看作是一個搜尋問題，利用一個預測模型對特徵子集進行訓練和評估，從而決定哪些特徵應該被保留。*」這就是包裹法的核心思想。它將模型的性能作為特徵子集好壞的直接衡量標準，而非像過濾法那樣僅依賴統計量。\n",
    "\n",
    "### 包裹法的優點與缺點\n",
    "\n",
    "- **優點**:\n",
    "  - **高相關性**: 由於直接針對特定模型的效能進行優化，選出的特徵與該模型的預測能力高度相關。\n",
    "  - **考慮特徵交互**: 包裹法能夠評估特徵之間的組合效應，這是過濾法難以做到的。\n",
    "\n",
    "- **缺點**:\n",
    "  - **計算成本高**: 需要重複訓練模型，當特徵數量龐大時，計算成本非常高昂。這使得它在大規模資料集上應用受限。\n",
    "  - **容易過擬合**: 如果評估指標選擇不當或資料集太小，可能會選出僅在訓練集上表現良好的特徵，導致模型在未知數據上泛化能力差。\n",
    "\n",
    "---\n",
    "\n",
    "## 常用包裹法技術\n",
    "\n",
    "1. **前向特徵選擇 (Step Forward Feature Selection, SFS)**\n",
    "2. **後向特徵消除 (Backward Feature Elimination, BFE)**\n",
    "3. **遞歸特徵消除 (Recursive Feature Elimination, RFE)**\n",
    "\n",
    "現在，我們將使用 `mlxtend` 和 `scikit-learn` 套件來實作這些方法。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8285c097",
   "metadata": {},
   "source": [
    "## 1. 載入套件與資料\n",
    "\n",
    "我們將使用 `scikit-learn` 內建的糖尿病資料集來示範。這個資料集包含 10 個基準線變數、年齡、性別、BMI、平均血壓和六種血清測量值，以及一年後疾病進展的定量指標。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85127c04",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.metrics import r2_score\n",
    "from mlxtend.feature_selection import SequentialFeatureSelector as SFS\n",
    "from sklearn.datasets import load_diabetes\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "\n",
    "# 設定視覺化風格\n",
    "sns.set_style('whitegrid')\n",
    "\n",
    "# 載入糖尿病資料集\n",
    "diabetes = load_diabetes()\n",
    "X = pd.DataFrame(diabetes.data, columns=diabetes.feature_names)\n",
    "y = pd.Series(diabetes.target, name='target')\n",
    "\n",
    "# 切分訓練集與測試集\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "print(\"資料載入與切分完成！\")\n",
    "print(\"X_train shape:\", X_train.shape)\n",
    "print(\"X_test shape:\", X_test.shape)\n",
    "\n",
    "# 顯示前五筆資料\n",
    "print(\"\\nX_train 前五筆：\")\n",
    "display(X_train.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6de6f9d5",
   "metadata": {},
   "source": [
    "## 2. 前向特徵選擇 (Step Forward Feature Selection - SFS)\n",
    "\n",
    "前向選擇從一個空集合開始，每次迭代都從剩餘的特徵中選擇一個能最大程度提升模型效能的特徵加入子集，直到達到預設的特徵數量或模型效能不再顯著提升為止。\n",
    "\n",
    "**優點**：概念直觀，計算成本相對後向消除低。\n",
    "**缺點**：容易陷入局部最優，一旦加入的特徵就不能移除，可能錯過最佳的特徵組合。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42d9e0c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化隨機森林回歸模型\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42, n_jobs=-1)\n",
    "\n",
    "# 初始化前向選擇器\n",
    "# k_features='best' 會自動找到最佳特徵數\n",
    "sfs = SFS(estimator=rf, \n",
    "          k_features='best', # 可以設定為(1, X_train.shape[1])來搜尋最佳特徵數\n",
    "          forward=True, \n",
    "          floating=False, # 是否允許特徵在加入後被移除 (用於浮動選擇)\n",
    "          scoring='r2', # 評估指標\n",
    "          cv=3, # 交叉驗證折數\n",
    "          n_jobs=-1) # 使用所有可用的CPU核心\n",
    "\n",
    "print(\"正在執行前向特徵選擇，這可能需要一些時間...\")\n",
    "# 訓練選擇器\n",
    "sfs = sfs.fit(X_train, y_train)\n",
    "\n",
    "print(\"前向特徵選擇完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f0ff6fe",
   "metadata": {},
   "source": [
    "**結果解讀**：\n",
    "\n",
    "訓練完成後，我們可以查看 SFS 選擇的最佳特徵子集及其對應的模型效能分數。`k_feature_names_` 屬性會返回所選特徵的名稱，而 `k_score_` 則表示該特徵子集在交叉驗證上的 R2 分數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1774442",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看選擇的最佳特徵\n",
    "selected_features_sfs = list(sfs.k_feature_names_)\n",
    "print(f\"SFS 選擇的最佳特徵 ({sfs.k_score_:.4f} R2 score):\")\n",
    "print(selected_features_sfs)\n",
    "\n",
    "# 建立只包含選中特徵的資料集\n",
    "X_train_sfs = sfs.transform(X_train)\n",
    "X_test_sfs = sfs.transform(X_test)\n",
    "\n",
    "# 使用選擇的特徵重新訓練模型並評估\n",
    "rf_sfs = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_sfs.fit(X_train_sfs, y_train)\n",
    "y_pred_sfs = rf_sfs.predict(X_test_sfs)\n",
    "r2_sfs = r2_score(y_test, y_pred_sfs)\n",
    "\n",
    "print(f\"\\n使用SFS選擇的 {len(selected_features_sfs)} 個特徵，在測試集上的 R2 Score: {r2_sfs:.4f}\")\n",
    "\n",
    "# 比較所有特徵的效能\n",
    "rf_full = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_full.fit(X_train, y_train)\n",
    "y_pred_full = rf_full.predict(X_test)\n",
    "r2_full = r2_score(y_test, y_pred_full)\n",
    "\n",
    "print(f\"使用全部 {X_train.shape[1]} 個特徵，在測試集上的 R2 Score: {r2_full:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2adba26",
   "metadata": {},
   "source": [
    "**討論**：\n",
    "\n",
    "從結果可以看出，SFS 選擇了一個較小的特徵子集，但在測試集上的 R2 分數可能與使用所有特徵的模型接近甚至更優。這說明 SFS 成功地移除了冗餘或不重要的特徵，有助於提高模型的泛化能力。\n",
    "\n",
    "## 3. 後向特徵消除 (Backward Feature Elimination - BFE)\n",
    "\n",
    "後向消除從包含所有特徵的集合開始，每次迭代都移除一個對模型效能影響最小的特徵，直到達到預設的特徵數量。\n",
    "\n",
    "**優點**：能考慮到所有特徵之間的交互作用，通常能找到更好的特徵子集。\n",
    "**缺點**：計算成本非常高，因為每次迭代都要重新訓練模型，且從所有特徵開始。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b8447ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 初始化後向消除器\n",
    "# 同樣，我們讓它自動找到最佳特徵組合\n",
    "sbs = SFS(estimator=rf, \n",
    "          k_features='best', # 尋找最佳特徵組合\n",
    "          forward=False,      # backward=True\n",
    "          floating=False, \n",
    "          scoring='r2',\n",
    "          cv=3,\n",
    "          n_jobs=-1)\n",
    "\n",
    "print(\"正在執行後向特徵消除，這可能需要一些時間...\")\n",
    "# 訓練選擇器\n",
    "sbs = sbs.fit(X_train, y_train)\n",
    "\n",
    "print(\"後向特徵消除完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c9208ed",
   "metadata": {},
   "source": [
    "**結果解讀**：\n",
    "\n",
    "類似於 SFS，我們查看 BFE 選擇的最佳特徵及其 R2 分數。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0786c747",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看選擇的最佳特徵\n",
    "selected_features_sbs = list(sbs.k_feature_names_)\n",
    "print(f\"BFE 選擇的最佳特徵 ({sbs.k_score_:.4f} R2 score):\")\n",
    "print(selected_features_sbs)\n",
    "\n",
    "# 建立只包含選中特徵的資料集\n",
    "X_train_sbs = sbs.transform(X_train)\n",
    "X_test_sbs = sbs.transform(X_test)\n",
    "\n",
    "# 使用選擇的特徵重新訓練模型並評估\n",
    "rf_sbs = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_sbs.fit(X_train_sbs, y_train)\n",
    "y_pred_sbs = rf_sbs.predict(X_test_sbs)\n",
    "r2_sbs = r2_score(y_test, y_pred_sbs)\n",
    "\n",
    "print(f\"\\n使用BFE選擇的 {len(selected_features_sbs)} 個特徵，在測試集上的 R2 Score: {r2_sbs:.4f}\")\n",
    "print(f\"使用全部 {X_train.shape[1]} 個特徵，在測試集上的 R2 Score: {r2_full:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "304377cb",
   "metadata": {},
   "source": [
    "**討論**：\n",
    "\n",
    "BFE 的結果通常會比 SFS 更優或持平，因為它在起始點考慮了所有特徵的上下文。然而，其高昂的計算成本是其主要缺點。\n",
    "\n",
    "### 視覺化選擇過程\n",
    "\n",
    "`mlxtend` 提供了便利的視覺化工具來展示特徵選擇過程中的效能變化。這有助於我們直觀地理解不同特徵數量下模型效能的趨勢。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71a3bfd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "from mlxtend.plotting import plot_sequential_feature_selection as plot_sfs\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(16, 6))\n",
    "\n",
    "# 繪製 SFS\n",
    "plot_sfs(sfs.get_metric_dict(), kind='std_dev', ax=ax1)\n",
    "ax1.set_title('Sequential Forward Selection (w. StdErr)')\n",
    "ax1.set_ylabel('R2 Score (CV Average)')\n",
    "ax1.set_xlabel('Number of Features')\n",
    "ax1.grid()\n",
    "\n",
    "# 繪製 SBS\n",
    "plot_sfs(sbs.get_metric_dict(), kind='std_dev', ax=ax2)\n",
    "ax2.set_title('Sequential Backward Selection (w. StdErr)')\n",
    "ax2.set_ylabel('R2 Score (CV Average)')\n",
    "ax2.set_xlabel('Number of Features')\n",
    "ax2.grid()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "befa2f7d",
   "metadata": {},
   "source": [
    "**圖表解讀**：\n",
    "\n",
    "這些圖表顯示了隨著選擇的特徵數量增加（SFS）或減少（BFE），模型在交叉驗證上的平均 R2 分數如何變化。陰影區域表示標準差。我們可以從圖中找到 R2 分數達到高峰的特徵數量，作為選擇最佳特徵數的參考。\n",
    "\n",
    "## 4. 遞歸特徵消除 (Recursive Feature Elimination - RFE)\n",
    "\n",
    "RFE 是一種更為複雜的包裹法。它首先用所有特徵訓練一個模型，然後根據特徵的重要性（例如，回歸係數或特徵重要性分數）移除最不重要的特徵。這個過程會不斷重複，直到剩下預定數量的特徵。\n",
    "\n",
    "`scikit-learn` 提供了 `RFE` 和 `RFECV` (帶交叉驗證的RFE) 兩種實現。\n",
    "\n",
    "**優點**：考慮了特徵間的相互作用和重要性，可以自動調整特徵數量。\n",
    "**缺點**：計算成本仍然較高，且結果依賴於內部模型的特徵重要性計算穩定性。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c85957c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFE\n",
    "\n",
    "# 初始化模型\n",
    "rf = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "\n",
    "# 初始化RFE\n",
    "# n_features_to_select: 指定要選擇的特徵數量\n",
    "# 我們可以先選擇一半的特徵，例如 5 個特徵\n",
    "rfe = RFE(estimator=rf, n_features_to_select=5, step=1) # step=1 表示每次迭代移除一個特徵\n",
    "\n",
    "print(\"正在執行 RFE...\")\n",
    "# 訓練RFE\n",
    "rfe.fit(X_train, y_train)\n",
    "\n",
    "print(\"RFE 完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "def104c9",
   "metadata": {},
   "source": [
    "**結果解讀**：\n",
    "\n",
    "RFE 會提供一個特徵排名，排名為 1 的特徵是被選中的。我們可以查看這些被選中的特徵以及所有特徵的排名。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b260c154",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 查看RFE的選擇結果\n",
    "selected_features_rfe = X_train.columns[rfe.support_]\n",
    "print(\"RFE 選擇的特徵:\")\n",
    "print(selected_features_rfe.tolist())\n",
    "\n",
    "# ranking_ 屬性顯示了特徵的排名，1表示被選中\n",
    "print(\"\\n特徵排名 (1表示被選中，數字越大重要性越低):\")\n",
    "rfe_ranking = pd.Series(rfe.ranking_, index=X_train.columns)\n",
    "display(rfe_ranking.sort_values())\n",
    "\n",
    "# 使用選擇的特徵進行評估\n",
    "X_train_rfe = rfe.transform(X_train)\n",
    "X_test_rfe = rfe.transform(X_test)\n",
    "\n",
    "rf_rfe = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "rf_rfe.fit(X_train_rfe, y_train)\n",
    "y_pred_rfe = rf_rfe.predict(X_test_rfe)\n",
    "r2_rfe = r2_score(y_test, y_pred_rfe)\n",
    "\n",
    "print(f\"\\n使用RFE選擇的 {len(selected_features_rfe)} 個特徵，在測試集上的 R2 Score: {r2_rfe:.4f}\")\n",
    "print(f\"使用全部 {X_train.shape[1]} 個特徵，在測試集上的 R2 Score: {r2_full:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e68e3237",
   "metadata": {},
   "source": [
    "**討論**：\n",
    "\n",
    "RFE 能夠根據模型內部的特徵重要性來進行選擇，這使得它在很多情況下表現出色。我們可以透過調整 `n_features_to_select` 來控制最終選出的特徵數量。\n",
    "\n",
    "### 使用 RFECV 自動選擇最佳特徵數\n",
    "\n",
    "`RFECV` 透過交叉驗證來自動尋找最佳的特徵數量，避免了手動設定 `n_features_to_select` 的麻煩。這是 RFE 的一個更實用的版本。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44defbb1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import RFECV\n",
    "\n",
    "# 初始化模型\n",
    "rf = RandomForestRegressor(n_estimators=50, random_state=42, n_jobs=-1) # 使用較少估計器加速，以便範例運行更快\n",
    "\n",
    "# 初始化RFECV\n",
    "# cv: 交叉驗證折數，用於評估不同特徵數量下的模型性能\n",
    "# scoring: 評估指標，這裡使用R2分數\n",
    "rfecv = RFECV(estimator=rf, \n",
    "              step=1, \n",
    "              cv=5, \n",
    "              scoring='r2',\n",
    "              n_jobs=-1)\n",
    "\n",
    "print(\"正在執行 RFECV，這將自動尋找最佳特徵數量...\")\n",
    "# 訓練RFECV\n",
    "rfecv.fit(X_train, y_train)\n",
    "\n",
    "print(\"RFECV 完成！\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a020f9e",
   "metadata": {},
   "source": [
    "**結果解讀**：\n",
    "\n",
    "RFECV 會告訴我們它認為最佳的特徵數量，並提供每個特徵的 `support_` 屬性來標示是否被選中。最重要的是，我們可以透過繪製交叉驗證分數圖來觀察模型性能隨特徵數量的變化。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8628bd64",
   "metadata": {},
   "outputs": [],
   "source": [
    "# RFECV 結果\n",
    "print(f\"RFECV 找到的最佳特徵數量: {rfecv.n_features_}\")\n",
    "\n",
    "selected_features_rfecv = X_train.columns[rfecv.support_]\n",
    "print(\"\\nRFECV 選擇的最佳特徵:\")\n",
    "print(selected_features_rfecv.tolist())\n",
    "\n",
    "# 視覺化交叉驗證分數\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.xlabel(\"Number of features selected\")\n",
    "plt.ylabel(\"Cross validation score (R^2)\")\n",
    "plt.plot(range(1, len(rfecv.cv_results_['mean_test_score']) + 1), rfecv.cv_results_['mean_test_score'], marker='o')\n",
    "plt.title(\"Recursive Feature Elimination with Cross-Validation\")\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# 使用RFECV選擇的特徵進行最終評估\n",
    "X_train_rfecv = rfecv.transform(X_train)\n",
    "X_test_rfecv = rfecv.transform(X_test)\n",
    "\n",
    "# 這裡我們可以直接使用已經訓練好的rfecv.estimator_來對原始X_test進行預測 (因為rfecv內部會處理轉換)\n",
    "y_pred_rfecv = rfecv.predict(X_test) \n",
    "r2_rfecv = r2_score(y_test, y_pred_rfecv)\n",
    "\n",
    "print(f\"\\n使用RFECV選擇的 {rfecv.n_features_} 個特徵，在測試集上的 R2 Score: {r2_rfecv:.4f}\")\n",
    "print(f\"使用全部 {X_train.shape[1]} 個特徵，在測試集上的 R2 Score: {r2_full:.4f}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c9302e8",
   "metadata": {},
   "source": [
    "**討論**：\n",
    "\n",
    "RFECV 圖表清楚地展示了模型性能如何隨著特徵數量的增加而變化。我們可以從圖中找到最佳的特徵數量，即 R2 分數達到最高的點。在很多情況下，特徵數量較少但性能相近的子集是更優的選擇。\n",
    "\n",
    "## 5. 總結\n",
    "\n",
    "在這個筆記本中，我們探討並實作了三種主要的包裹法特徵選擇技術：\n",
    "\n",
    "1.  **前向特徵選擇 (SFS)**: 從零開始，逐步加入最有用的特徵。優點是概念直觀，計算成本相對較低（相對於從所有特徵開始的方法）。缺點是可能陷入局部最優。\n",
    "2.  **後向特徵消除 (BFE)**: 從全部特徵開始，逐步移除最無用的特徵。優點是能評估特徵組合的整體效果。缺點是計算成本非常高。\n",
    "3.  **遞歸特徵消除 (RFE/RFECV)**: 透過模型的特徵重要性，迭代地移除特徵。優點是考慮了特徵間的相互作用和重要性，並且 RFECV 能自動找到最佳特徵數量。缺點是計算成本仍然較高，且結果依賴於模型的穩定性。\n",
    "\n",
    "| 方法 | 優點 | 缺點 |\n",
    "|:---|:---|:---|\n",
    "| **SFS** | 計算成本相對較低（與BFE相比），直觀 | 可能陷入局部最優，錯過特徵組合 |\n",
    "| **BFE** | 能評估特徵組合的整體效果，考慮所有特徵上下文 | 計算成本非常高，速度慢 |\n",
    "| **RFE/RFECV**| 考慮了特徵間的相互作用和重要性，RFECV可自動優化特徵數量 | 計算成本高，且結果依賴於模型的穩定性 |\n",
    "\n",
    "包裹法雖然計算成本較高，但由於其直接與模型效能掛鉤，通常能找到比過濾法更好的特徵子集。在實際應用中，可以先使用過濾法進行初步篩選，再用包裹法進行精細選擇，以平衡效果與效率。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
