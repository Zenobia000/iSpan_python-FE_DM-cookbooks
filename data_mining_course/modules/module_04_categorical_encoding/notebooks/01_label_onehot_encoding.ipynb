{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5a8430d4",
   "metadata": {},
   "source": [
    "# 模組 4.1: 標籤編碼與獨熱編碼 (Label & One-Hot Encoding)\n",
    "\n",
    "## 學習目標\n",
    "- 理解名目 (Nominal) 與順序 (Ordinal) 類別資料的區別。\n",
    "- 學習如何實作標籤編碼 (Label Encoding) 及其適用場景。\n",
    "- 學習如何實作獨熱編碼 (One-Hot Encoding) 及其適用場景。\n",
    "- 分析並比較兩種方法的主要優缺點。\n",
    "\n",
    "## 導論：為何需要編碼？\n",
    "\n",
    "在您的指南中提到：「*將類別型特徵（通常是文本標籤）轉換為機器學習演算法能夠處理的數值格式*」。這是類別變數編碼的根本目的。大多數演算法，特別是基於數學方程式的模型（如線性迴歸、邏輯迴歸、SVM），無法直接理解 \"Male\", \"Female\" 或 \"S\", \"C\", \"Q\" 這些字串。我們必須將它們轉換為數字。\n",
    "\n",
    "本筆記本將介紹兩種最基礎的編碼方式，它們的選擇與特徵本身的性質（順序或名目）息息相關。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6897e598",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的函式庫\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import LabelEncoder, OneHotEncoder\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cdef24d0",
   "metadata": {},
   "source": [
    "## 1. 準備資料與概念區分\n",
    "\n",
    "我們創建一個簡單的 DataFrame 來區分兩種主要的類別資料類型。\n",
    "\n",
    "- **順序型 (Ordinal)**: 類別之間存在明確的內在順序或等級。例如：`低 < 中 < 高`。\n",
    "- **名目型 (Nominal)**: 類別之間僅為名稱不同，沒有任何順序關係。例如：`紅, 綠, 藍`。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3422ace",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 創建一個包含順序和名目特徵的 DataFrame\n",
    "df = pd.DataFrame({\n",
    "    'Color': ['Red', 'Green', 'Blue', 'Green', 'Red'],\n",
    "    'Size': ['M', 'L', 'S', 'M', 'L'],\n",
    "    'Rating': ['Good', 'Excellent', 'Good', 'Fair', 'Excellent']\n",
    "})\n",
    "\n",
    "print(\"原始 DataFrame:\")\n",
    "display(df)\n",
    "\n",
    "# 在這裡，'Color' 是名目型，而 'Size' 和 'Rating' 都是順序型。\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83d39fd6",
   "metadata": {},
   "source": [
    "## 2. 標籤編碼 (Label Encoding)\n",
    "\n",
    "**原理**: 為每個唯一類別分配一個從 0 開始的連續整數。例如 `['Red', 'Green', 'Blue']` 可能被編碼為 `[2, 1, 0]`。\n",
    "\n",
    "**適用場景**:\n",
    "1.  **順序型特徵**: 當類別本身就有順序時，這種編碼可以保留順序資訊。\n",
    "2.  **樹模型**: 決策樹、隨機森林、XGBoost 等模型能處理數值大小，通常不受標籤編碼引入的虛假順序影響，因此可以直接使用。\n",
    "\n",
    "**陷阱**: **絕對不要對名目型特徵使用標籤編碼後，再輸入到線性模型中！** 這會讓模型誤以為類別之間存在大小關係（例如，`Red(2) > Green(1)`），從而得出錯誤的結論。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c80e7d8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 對 'Size' (順序型) 進行標籤編碼 ---\n",
    "# 首先，我們需要手動定義正確的順序\n",
    "size_mapping = {'S': 0, 'M': 1, 'L': 2}\n",
    "df['Size_LabelEncoded'] = df['Size'].map(size_mapping)\n",
    "\n",
    "# --- 使用 scikit-learn 的 LabelEncoder ---\n",
    "# 注意：LabelEncoder 會按照字母順序分配編碼，不一定符合你的業務邏輯順序！\n",
    "le = LabelEncoder()\n",
    "df['Color_LabelEncoded'] = le.fit_transform(df['Color'])\n",
    "\n",
    "print(\"標籤編碼後的 DataFrame:\")\n",
    "display(df)\n",
    "\n",
    "print(\"\\nColor 欄位的編碼規則:\")\n",
    "# le.classes_ 可以看到編碼對應的原始類別\n",
    "for i, cls in enumerate(le.classes_):\n",
    "    print(f\"{cls} -> {i}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1984e5",
   "metadata": {},
   "source": [
    "## 3. 獨熱編碼 (One-Hot Encoding)\n",
    "\n",
    "**原理**: 為每個類別創建一個新的二元 (0/1) 欄位。如果某個樣本屬於該類別，則對應欄位為 1，其餘為 0。\n",
    "\n",
    "**適用場景**:\n",
    "1.  **名目型特徵**: 這是處理名目型特徵最標準、最安全的方法，它不會引入任何虛假的順序關係。\n",
    "2.  **線性模型或基於距離的模型**: 對於邏輯迴歸、SVM、KNN 等對特徵數值大小敏感的模型，獨熱編碼是必要的。\n",
    "\n",
    "**陷阱**:\n",
    "- **維度災難**: 如果一個特徵的基數（唯一類別數量）非常高，獨熱編碼會產生大量新欄位，增加計算成本和模型複雜度。\n",
    "- **共線性**: 產生的新欄位是線性相關的（例如，如果不是男性就一定是女性）。可以通過設置 `drop='first'` 來移除第一個類別的欄位以避免這種情況，但現在大多數模型庫都能自動處理。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2919952a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 使用 Pandas 的 get_dummies 進行獨熱編碼 ---\n",
    "# get_dummies 是最方便的獨熱編碼方法\n",
    "df_onehot = pd.get_dummies(df[['Color', 'Rating']], prefix=['Color', 'Rating'])\n",
    "\n",
    "# 將編碼結果與原 DataFrame 合併\n",
    "df_final = pd.concat([df, df_onehot], axis=1)\n",
    "\n",
    "print(\"獨熱編碼後的 DataFrame:\")\n",
    "display(df_final)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08a0c7d6",
   "metadata": {},
   "source": [
    "## 4. 方法比較與總結\n",
    "\n",
    "| 方法 | 原理 | 優點 | 缺點/風險 | 適用模型/場景 |\n",
    "| :--- | :--- | :--- | :--- | :--- |\n",
    "| **標籤編碼** | `Red`->0, `Green`->1 | 不增加特徵維度，計算簡單。 | 對名目型特徵引入錯誤順序。 | **順序型特徵**；**樹模型** (Random Forest, XGBoost)。 |\n",
    "| **獨熱編碼** | `Red`->`[1,0,0]` | **避免引入錯誤順序**，適用性廣。 | **維度災難** (高基數特徵)；可能產生共線性。 | **名目型特徵**；**線性模型** (Logistic Regression, SVM), **基於距離的模型** (KNN)。 |\n",
    "\n",
    "**選擇的核心原則**:\n",
    "- 你的特徵是 **順序型** 還是 **名目型**？\n",
    "- 你使用的 **模型** 是否對數值大小敏感？\n",
    "\n",
    "回答這兩個問題，就能做出正確的選擇。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
