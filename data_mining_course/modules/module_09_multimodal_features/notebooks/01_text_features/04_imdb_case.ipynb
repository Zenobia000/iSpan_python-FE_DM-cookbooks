{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "1f35a163",
   "metadata": {},
   "source": [
    "# Module 9: 多模態特徵工程 - 4. 文本特徵工程：IMDB 影評情感分析案例\n",
    "\n",
    "## 學習目標\n",
    "- 在一個真實的二元情感分類資料集（IMDB 電影評論）上，綜合應用所學的文本特徵工程技術。\n",
    "- 學習如何載入和初步探索大型文本資料集。\n",
    "- 掌握文本預處理的標準流程，包括 HTML 標籤移除、標點符號清理、小寫轉換、分詞和停用詞移除。\n",
    "- 實作 TF-IDF 特徵提取，將文本轉換為數值表示。\n",
    "- 學習如何進行文本資料的訓練/測試集分割，並避免常見的數據洩漏問題。\n",
    "- 訓練並評估一個基於文本特徵的邏輯回歸情感分類模型。\n",
    "- 理解文本特徵工程在實際情感分析任務中的應用和挑戰。\n",
    "\n",
    "## 導論：如何讓機器學習模型理解電影評論的「情感」？\n",
    "\n",
    "在數位時代，人們透過評論、推文和貼文表達對產品、服務或事件的看法。從這些非結構化文本中自動判斷其情感極性（例如，正面、負面或中立）是一項重要的自然語言處理 (NLP) 任務，廣泛應用於客戶服務、市場分析和輿情監控。本案例研究旨在將 `Module 9` 中文本特徵工程部分的知識——包括文本預處理、詞袋模型和 TF-IDF 特徵提取——綜合應用於一個經典的 NLP 問題：**基於 IMDB 電影評論進行情感分析**。\n",
    "\n",
    "您的指南強調「文本特徵工程旨在將非結構化文本數據轉化為數值特徵，以供模型學習」。在這個案例中，我們將面對包含大量電影評論文本的資料集，這些評論通常包含噪音（如 HTML 標籤）、不規則的標點符號，以及對情感判斷無益的停用詞。我們將學習如何將這些原始文本清理並轉換為機器學習模型能夠理解的數值特徵，進而訓練一個分類器來判斷評論是正面的還是負面的。\n",
    "\n",
    "**這個案例將展示：**\n",
    "- 如何處理真實世界的文本資料，從檔案讀取到 DataFrame 結構。\n",
    "- 文本預處理的每一個關鍵步驟如何應用。\n",
    "- 如何運用 TF-IDF 將清洗後的文本轉換為有效的數值特徵。\n",
    "- 如何建立一個端到端的情感分析模型，並評估其性能。\n",
    "- 情感分析在實際場景中的應用潛力。\n",
    "\n",
    "---\n",
    "\n",
    "## 1. 資料準備與套件載入：情感分析的基石\n",
    "\n",
    "在開始文本特徵工程之前，我們需要載入必要的 Python 套件，並準備 IMDB 電影評論資料集。這個資料集通常以多個文本檔案的形式組織，需要我們手動讀取並整合到 Pandas DataFrame 中。同時，我們將處理 NLTK 相關資源的下載，確保文本預處理工具可用。\n",
    "\n",
    "**請注意**：\n",
    "1.  IMDB 資料集預設儲存路徑為 `../../datasets/raw/imdb_reviews/`。請確保您已從 [Kaggle](https://www.kaggle.com/datasets/lakshmi25npathi/imdb-dataset-of-50k-movie-reviews) 下載並解壓縮，使其包含 `train/pos` 和 `train/neg` 等子資料夾。\n",
    "2.  本筆記本需要 `nltk` 庫，如果尚未安裝，請執行 `pip install nltk`。同時，NLTK 的停用詞和分詞器資源（如 `stopwords`, `punkt`）可能需要首次下載。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "df0960b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://mirrors.aliyun.com/pypi/simple/\n",
      "Requirement already satisfied: nltk in c:\\users\\xdxd2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (3.9.1)\n",
      "Requirement already satisfied: click in c:\\users\\xdxd2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (8.1.7)\n",
      "Requirement already satisfied: joblib in c:\\users\\xdxd2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (1.4.2)\n",
      "Requirement already satisfied: regex>=2021.8.3 in c:\\users\\xdxd2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (2024.11.6)\n",
      "Requirement already satisfied: tqdm in c:\\users\\xdxd2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from nltk) (4.67.1)\n",
      "Requirement already satisfied: colorama in c:\\users\\xdxd2\\appdata\\local\\programs\\python\\python311\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.1.1 -> 25.1.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "2e2098a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== 解決 IMDB 資料集載入問題 ===\n",
      "當前工作目錄：d:\\python_workspace\\project_dataAnalysis\\iSpan_python-FE_DM-cookbooks\\data_mining_course\\modules\\module_09_multimodal_features\\notebooks\\01_text_features\n",
      "嘗試路徑 1: ../../../../datasets/raw/imdb_reviews/IMDB Dataset.csv\n",
      "✗ 檔案存在但無權限存取：../../../../datasets/raw/imdb_reviews/IMDB Dataset.csv\n",
      "嘗試路徑 2: datasets/raw/imdb_reviews/IMDB Dataset.csv\n",
      "✗ 檔案不存在：datasets/raw/imdb_reviews/IMDB Dataset.csv\n",
      "嘗試路徑 3: d:\\python_workspace\\project_dataAnalysis\\iSpan_python-FE_DM-cookbooks\\data_mining_course\\modules\\module_09_multimodal_features\\notebooks\\01_text_features\\datasets\\raw\\imdb_reviews\\IMDB Dataset.csv\n",
      "✗ 檔案不存在：d:\\python_workspace\\project_dataAnalysis\\iSpan_python-FE_DM-cookbooks\\data_mining_course\\modules\\module_09_multimodal_features\\notebooks\\01_text_features\\datasets\\raw\\imdb_reviews\\IMDB Dataset.csv\n",
      "嘗試路徑 4: d:\\python_workspace\\project_dataAnalysis\\iSpan_python-FE_DM-cookbooks\\data_mining_course\\datasets\\raw\\imdb_reviews\\IMDB Dataset.csv\n",
      "✗ 檔案存在但無權限存取：d:\\python_workspace\\project_dataAnalysis\\iSpan_python-FE_DM-cookbooks\\data_mining_course\\datasets\\raw\\imdb_reviews\\IMDB Dataset.csv\n",
      "\n",
      "❌ 無法找到可存取的 IMDB 資料集檔案\n",
      "\n",
      "可能的解決方案：\n",
      "1. 關閉所有可能開啟該檔案的程序（特別是 Excel）\n",
      "2. 檢查檔案是否被防毒軟體鎖定\n",
      "3. 以管理員身份執行 Jupyter Notebook\n",
      "4. 檢查檔案權限設定\n"
     ]
    }
   ],
   "source": [
    "# --- 解決路徑和權限問題 ---\n",
    "import os\n",
    "import time\n",
    "\n",
    "print(\"=== 解決 IMDB 資料集載入問題 ===\")\n",
    "print(f\"當前工作目錄：{os.getcwd()}\")\n",
    "\n",
    "# 嘗試不同的路徑選項\n",
    "possible_paths = [\n",
    "    r\"../../../../datasets/raw/imdb_reviews/IMDB Dataset.csv\",\n",
    "    r\"datasets/raw/imdb_reviews/IMDB Dataset.csv\", \n",
    "    os.path.join(os.getcwd(), \"datasets\", \"raw\", \"imdb_reviews\", \"IMDB Dataset.csv\"),\n",
    "    os.path.abspath(os.path.join(os.getcwd(), \"..\", \"..\", \"..\", \"..\", \"datasets\", \"raw\", \"imdb_reviews\", \"IMDB Dataset.csv\")),\n",
    "]\n",
    "\n",
    "DATA_DIR = None\n",
    "for i, path in enumerate(possible_paths):\n",
    "    print(f\"嘗試路徑 {i+1}: {path}\")\n",
    "    if os.path.exists(path):\n",
    "        # 檢查檔案是否可讀取\n",
    "        try:\n",
    "            # 嘗試開啟檔案進行讀取測試\n",
    "            with open(path, 'r', encoding='utf-8') as test_file:\n",
    "                test_file.read(100)  # 讀取前100個字符測試\n",
    "            DATA_DIR = path\n",
    "            print(f\"✓ 成功找到可存取的資料集檔案：{path}\")\n",
    "            break\n",
    "        except PermissionError:\n",
    "            print(f\"✗ 檔案存在但無權限存取：{path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"✗ 檔案存取測試失敗：{e}\")\n",
    "    else:\n",
    "        print(f\"✗ 檔案不存在：{path}\")\n",
    "\n",
    "if DATA_DIR is None:\n",
    "    print(\"\\n❌ 無法找到可存取的 IMDB 資料集檔案\")\n",
    "    print(\"\\n可能的解決方案：\")\n",
    "    print(\"1. 關閉所有可能開啟該檔案的程序（特別是 Excel）\")\n",
    "    print(\"2. 檢查檔案是否被防毒軟體鎖定\")\n",
    "    print(\"3. 以管理員身份執行 Jupyter Notebook\")\n",
    "    print(\"4. 檢查檔案權限設定\")\n",
    "else:\n",
    "    print(f\"\\n✓ 已設定資料路徑：{DATA_DIR}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "95df3d46",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "下載最新的 NLTK 資源...\n",
      "分詞測試成功: ['This', 'is', 'a', 'test', 'sentence', '.']\n",
      "停用詞載入成功，總數: 198\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt_tab to\n",
      "[nltk_data]     C:\\Users\\xdxd2/nltk_data...\n",
      "[nltk_data]   Package punkt_tab is already up-to-date!\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\xdxd2/nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import re\n",
    "import nltk # 自然語言處理庫\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import accuracy_score, classification_report\n",
    "\n",
    "# 設定視覺化風格\n",
    "sns.set_style('whitegrid')\n",
    "plt.rcParams['figure.figsize'] = (10, 6)\n",
    "\n",
    "# --- 配置資料路徑和 NLTK 數據路徑 ---\n",
    "DATA_DIR = r\"../../../../../datasets/raw/imdb_reviews/IMDB Dataset.csv\"\n",
    "# NLTK 數據的儲存路徑，通常設定為用戶家目錄下的 'nltk_data'\n",
    "nltk_data_path = os.path.join(os.path.expanduser(\"~\"), \"nltk_data\")\n",
    "\n",
    "\n",
    "print('下載最新的 NLTK 資源...')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('stopwords')\n",
    "\n",
    "# 測試是否能正常使用\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "# 測試分詞\n",
    "test_text = 'This is a test sentence.'\n",
    "tokens = word_tokenize(test_text)\n",
    "print(f'分詞測試成功: {tokens}')\n",
    "\n",
    "# 測試停用詞\n",
    "stop_words = set(stopwords.words('english'))\n",
    "print(f'停用詞載入成功，總數: {len(stop_words)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "fe41973c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在載入 IMDB 資料集...\n",
      "檢測到的欄位：['review', 'sentiment']\n",
      "情感標籤的唯一值：['positive' 'negative']\n",
      "成功載入 50000 條評論\n",
      "正面評論數量：25000\n",
      "負面評論數量：25000\n",
      "\\n已成功載入 50000 條評論。\n",
      "資料集前3筆評論：\n",
      "                                              review  sentiment\n",
      "0  One of the other reviewers has mentioned that ...          1\n",
      "1  A wonderful little production. <br /><br />The...          1\n",
      "2  I thought this was a wonderful way to spend ti...          1\n"
     ]
    }
   ],
   "source": [
    "def load_imdb_data(csv_path):\n",
    "    print('正在載入 IMDB 資料集...')\n",
    "    \n",
    "    # 載入 CSV 檔案\n",
    "    df = pd.read_csv(csv_path)\n",
    "    \n",
    "    # 檢查欄位\n",
    "    print(f'檢測到的欄位：{list(df.columns)}')\n",
    "    print(f'情感標籤的唯一值：{df.sentiment.unique()}')\n",
    "    \n",
    "    # 將情感標籤轉換為數值格式 (positive -> 1, negative -> 0)\n",
    "    df['sentiment'] = df['sentiment'].apply(lambda x: 1 if x.lower() == 'positive' else 0)\n",
    "    \n",
    "    print(f'成功載入 {len(df)} 條評論')\n",
    "    print(f'正面評論數量：{df.sentiment.sum()}')\n",
    "    print(f'負面評論數量：{len(df) - df.sentiment.sum()}')\n",
    "    \n",
    "    return df\n",
    "\n",
    "# 載入資料\n",
    "df = load_imdb_data(DATA_DIR)\n",
    "\n",
    "print(f'\\\\n已成功載入 {len(df)} 條評論。')\n",
    "print('資料集前3筆評論：')\n",
    "print(df.head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd41913",
   "metadata": {},
   "source": [
    "\n",
    "**結果解讀**：\n",
    "\n",
    "我們已經成功載入了 IMDB 電影評論資料集，它包含原始評論文本 (`review`) 和對應的情感標籤 (`sentiment`，0 為負面，1 為正面）。資料集的大小和內容表明它是一個適合進行文本分類任務的基準數據集。接下來，我們將對這些原始文本進行必要的預處理。\n",
    "\n",
    "## 2. 文本預處理：將原始文本轉化為乾淨的詞語序列\n",
    "\n",
    "原始文本數據通常包含許多噪音和不一致性，例如 HTML 標籤、標點符號、大小寫混淆以及對模型無用的常用詞（停用詞）。文本預處理的目標是清理這些噪音，並將文本轉換為標準化的詞語序列 (tokens)，使其更適合機器學習模型進行特徵提取和學習。\n",
    "\n",
    "### 預處理步驟：\n",
    "1.  **移除 HTML 標籤**：電影評論中常見 `\\<br />` 等 HTML 標籤。\n",
    "2.  **移除非字母字符**：只保留字母，移除數字、特殊符號等。\n",
    "3.  **轉換為小寫**：統一所有文本的大小寫。\n",
    "4.  **分詞 (Tokenization)**：將文本分割成單個詞語。\n",
    "5.  **移除停用詞 (Stop Words Removal)**：移除像 \\\"the\\\", \\\"is\\\", \\\"and\\\" 等頻繁出現但缺乏實質語義的詞語。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "a5b81aba",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在進行文本預處理...\n",
      "文本預處理完成！\n",
      "清洗後的前5筆評論：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>review</th>\n",
       "      <th>cleaned_review</th>\n",
       "      <th>sentiment</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>One of the other reviewers has mentioned that ...</td>\n",
       "      <td>one reviewers mentioned watching oz episode yo...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>A wonderful little production. &lt;br /&gt;&lt;br /&gt;The...</td>\n",
       "      <td>wonderful little production filming technique ...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I thought this was a wonderful way to spend ti...</td>\n",
       "      <td>thought wonderful way spend time hot summer we...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Basically there's a family where a little boy ...</td>\n",
       "      <td>basically theres family little boy jake thinks...</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Petter Mattei's \"Love in the Time of Money\" is...</td>\n",
       "      <td>petter matteis love time money visually stunni...</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                              review  \\\n",
       "0  One of the other reviewers has mentioned that ...   \n",
       "1  A wonderful little production. <br /><br />The...   \n",
       "2  I thought this was a wonderful way to spend ti...   \n",
       "3  Basically there's a family where a little boy ...   \n",
       "4  Petter Mattei's \"Love in the Time of Money\" is...   \n",
       "\n",
       "                                      cleaned_review  sentiment  \n",
       "0  one reviewers mentioned watching oz episode yo...          1  \n",
       "1  wonderful little production filming technique ...          1  \n",
       "2  thought wonderful way spend time hot summer we...          1  \n",
       "3  basically theres family little boy jake thinks...          0  \n",
       "4  petter matteis love time money visually stunni...          1  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "print(\"正在進行文本預處理...\")\n",
    "def preprocess_text(text):\n",
    "    # 1. 移除 HTML 標籤\n",
    "    text = re.sub(r'<.*?>', '', text)\n",
    "    # 2. 移除非字母字符，只保留字母和空格 (re.I 忽略大小寫，re.A 匹配 ASCII 字符)\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', text, re.I|re.A)\n",
    "    # 3. 轉換為小寫\n",
    "    text = text.lower()\n",
    "    # 4. 分詞\n",
    "    tokens = word_tokenize(text)\n",
    "    # 5. 移除停用詞\n",
    "    filtered_tokens = [word for word in tokens if word not in stop_words and len(word) > 1] # 移除單個字母的詞\n",
    "    return \" \".join(filtered_tokens) # 將處理後的詞語重新組合為字符串\n",
    "\n",
    "# 僅在 df 不為空時執行預處理\n",
    "if not df.empty:\n",
    "    df['cleaned_review'] = df['review'].apply(preprocess_text)\n",
    "    print(\"文本預處理完成！\")\n",
    "    print(\"清洗後的前5筆評論：\")\n",
    "\n",
    "    display(df[['review', 'cleaned_review', 'sentiment']].head())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "068645ae",
   "metadata": {},
   "source": [
    "\n",
    "**結果解讀與討論**：\n",
    "\n",
    "`cleaned_review` 欄位現在包含了經過一系列預處理的文本：HTML 標籤已無，標點符號和數字已移除，所有字母均為小寫，並且文本已經分詞並移除了常見的停用詞和單個字母的詞。這些清洗後的文本更為簡潔，去除了噪音，使得後續的特徵提取器能夠更有效地從中學習有意義的模式，專注於那些真正攜帶情感信息的詞語。\n",
    "\n",
    "## 3. 資料分割：準備訓練與測試集\n",
    "\n",
    "在訓練機器學習模型之前，將資料集劃分為訓練集和測試集是標準且關鍵的步驟。對於文本分類任務，由於評論之間通常是獨立的，我們可以採用隨機分割。`stratify=y` 參數確保訓練集和測試集中情感類別的比例與原始資料集保持一致，這對於二元分類問題尤為重要，可以避免因類別不平衡導致模型訓練偏差。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "cbd34cce",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在分割資料集為訓練集和測試集...\n",
      "資料已成功分割為 40000 條訓練評論和 10000 條測試評論。\n",
      "訓練集中正面評論比例: 0.50\n",
      "測試集中正面評論比例: 0.50\n"
     ]
    }
   ],
   "source": [
    "print(\"正在分割資料集為訓練集和測試集...\")\n",
    "\n",
    "# 檢查資料集是否載入成功且包含清洗後的評論\n",
    "if not df.empty and 'cleaned_review' in df.columns:\n",
    "    # 定義特徵 (X) 和目標 (y)\n",
    "    X = df['cleaned_review'] # 清洗後的評論作為特徵\n",
    "    y = df['sentiment']      # 情感標籤作為目標\n",
    "    \n",
    "    # 劃分資料集\n",
    "    # test_size=0.2 表示 20% 的數據用於測試\n",
    "    # random_state=42 確保每次運行結果一致\n",
    "    # stratify=y 確保訓練集和測試集中 y 的分佈比例一致\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "    print(f\"資料已成功分割為 {len(X_train)} 條訓練評論和 {len(X_test)} 條測試評論。\")\n",
    "    print(f\"訓練集中正面評論比例: {y_train.sum() / len(y_train):.2f}\")\n",
    "    print(f\"測試集中正面評論比例: {y_test.sum() / len(y_test):.2f}\")\n",
    "else:\n",
    "    if df.empty:\n",
    "        print(\"資料集為空，無法進行分割。\")\n",
    "    else:\n",
    "        print(\"資料集中缺少 'cleaned_review' 欄位，請先執行文本預處理步驟。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e91b13e7",
   "metadata": {},
   "source": [
    "\n",
    "**討論**：\n",
    "\n",
    "資料分割確保了模型在訓練時只能看到訓練數據，從而能夠在未見過的測試數據上客觀地評估其泛化能力。`stratify=y` 的使用對於像情感分析這類可能存在類別不平衡的任務尤為重要，它保證了模型在訓練和測試階段都能面對相似的類別分佈，提高評估的可靠性。\n",
    "\n",
    "## 4. 特徵提取：TF-IDF 的應用\n",
    "\n",
    "在文本數據經過預處理和分割之後，下一步是將清洗後的文本轉換為機器學習模型可以理解的數值特徵。我們將使用 **TF-IDF (Term Frequency-Inverse Document Frequency)**，這是一種有效的文本表示方法，它能夠權衡詞語在單個文檔中的頻率和在整個語料庫中的稀有程度，從而賦予關鍵詞更高的權重。\n",
    "\n",
    "我們將使用 `scikit-learn` 的 `TfidfVectorizer`。請注意，它在內部會再次執行分詞和停止詞處理，所以我們的 `preprocess_text` 函數是為了更細緻的控制和演示。在實際應用中，通常會讓 `TfidfVectorizer` 自己處理大部分預處理步驟。\n",
    "\n",
    "### `TfidfVectorizer` 關鍵參數：\n",
    "-   `max_features`: 限制詞彙表的大小，只保留 TF-IDF 分數最高的 N 個詞。這有助於控制模型複雜度。\n",
    "-   `min_df`, `max_df`: 用於過濾過於稀有或過於常見的詞語。\n",
    "-   `ngram_range`: 可以設定為 (1, 2) 來包含二元詞組 (bigrams)，捕捉詞序信息。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "10d7c3f0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在使用 TF-IDF 提取文本特徵...\n",
      "TF-IDF 特徵提取完成！\n",
      "TF-IDF 特徵矩陣形狀 (訓練集): (40000, 5000)\n",
      "TF-IDF 特徵矩陣形狀 (測試集): (10000, 5000)\n",
      "部分 TF-IDF 特徵預覽 (訓練集)：\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>abandoned</th>\n",
       "      <th>abc</th>\n",
       "      <th>abilities</th>\n",
       "      <th>ability</th>\n",
       "      <th>able</th>\n",
       "      <th>absence</th>\n",
       "      <th>absent</th>\n",
       "      <th>absolute</th>\n",
       "      <th>absolutely</th>\n",
       "      <th>absurd</th>\n",
       "      <th>...</th>\n",
       "      <th>youll</th>\n",
       "      <th>young</th>\n",
       "      <th>younger</th>\n",
       "      <th>youre</th>\n",
       "      <th>youth</th>\n",
       "      <th>youve</th>\n",
       "      <th>zero</th>\n",
       "      <th>zombie</th>\n",
       "      <th>zombies</th>\n",
       "      <th>zone</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.058399</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.056492</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 5000 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   abandoned  abc  abilities  ability  able  absence  absent  absolute  \\\n",
       "0        0.0  0.0        0.0      0.0   0.0      0.0     0.0       0.0   \n",
       "1        0.0  0.0        0.0      0.0   0.0      0.0     0.0       0.0   \n",
       "2        0.0  0.0        0.0      0.0   0.0      0.0     0.0       0.0   \n",
       "3        0.0  0.0        0.0      0.0   0.0      0.0     0.0       0.0   \n",
       "4        0.0  0.0        0.0      0.0   0.0      0.0     0.0       0.0   \n",
       "\n",
       "   absolutely  absurd  ...  youll  young  younger     youre  youth  youve  \\\n",
       "0    0.000000     0.0  ...    0.0    0.0      0.0  0.000000    0.0    0.0   \n",
       "1    0.000000     0.0  ...    0.0    0.0      0.0  0.000000    0.0    0.0   \n",
       "2    0.000000     0.0  ...    0.0    0.0      0.0  0.000000    0.0    0.0   \n",
       "3    0.058399     0.0  ...    0.0    0.0      0.0  0.056492    0.0    0.0   \n",
       "4    0.000000     0.0  ...    0.0    0.0      0.0  0.000000    0.0    0.0   \n",
       "\n",
       "   zero  zombie  zombies  zone  \n",
       "0   0.0     0.0      0.0   0.0  \n",
       "1   0.0     0.0      0.0   0.0  \n",
       "2   0.0     0.0      0.0   0.0  \n",
       "3   0.0     0.0      0.0   0.0  \n",
       "4   0.0     0.0      0.0   0.0  \n",
       "\n",
       "[5 rows x 5000 columns]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"正在使用 TF-IDF 提取文本特徵...\")\n",
    "\n",
    "# 檢查是否有訓練集和測試集\n",
    "if 'X_train' in locals() and 'X_test' in locals() and 'y_train' in locals() and 'y_test' in locals():\n",
    "    # 初始化 TfidfVectorizer，限制詞彙表大小以控制維度\n",
    "    tfidf_vectorizer = TfidfVectorizer(max_features=5000) # 選擇最常見的 5000 個詞語作為特徵\n",
    "    \n",
    "    # 在訓練集上擬合 TF-IDF 模型，學習詞彙表和 IDF 權重\n",
    "    X_train_tfidf = tfidf_vectorizer.fit_transform(X_train)\n",
    "    # 使用訓練好的模型轉換測試集，**注意這裡只用 transform，不用 fit**\n",
    "    X_test_tfidf = tfidf_vectorizer.transform(X_test)\n",
    "    \n",
    "    print(\"TF-IDF 特徵提取完成！\")\n",
    "    print(f\"TF-IDF 特徵矩陣形狀 (訓練集): {X_train_tfidf.shape}\")\n",
    "    print(f\"TF-IDF 特徵矩陣形狀 (測試集): {X_test_tfidf.shape}\")\n",
    "    print(\"部分 TF-IDF 特徵預覽 (訓練集)：\")\n",
    "    display(pd.DataFrame(X_train_tfidf[:5].toarray(), columns=tfidf_vectorizer.get_feature_names_out()))\n",
    "else:\n",
    "    print(\"錯誤：未找到訓練集和測試集，請先執行資料分割步驟。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff4421d",
   "metadata": {},
   "source": [
    "\n",
    "**結果解讀與討論**：\n",
    "\n",
    "文本現在已經成功轉換為高維的 TF-IDF 稀疏矩陣。每個文檔（評論）都被表示為一個固定長度的向量，其中每個維度對應一個詞語的 TF-IDF 分數。`max_features=5000` 有效控制了特徵的維度，避免了維度災難。這些數值特徵將作為邏輯回歸模型的輸入。這種稀疏表示在處理文本數據時非常常見且高效。\n",
    "\n",
    "## 5. 模型訓練：情感分類器的構建\n",
    "\n",
    "在特徵提取完成後，我們將使用提取出的 TF-IDF 特徵來訓練一個情感分類模型。我們選擇 **邏輯回歸 (Logistic Regression)**，這是一個高效且解釋性強的線性分類器，在文本分類任務中常用作基準模型。\n",
    "\n",
    "`max_iter` 參數設定了優化算法的最大迭代次數，對於大型資料集或複雜模型，可能需要增加此值以確保模型收斂。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "71b30613",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在訓練邏輯回歸情感分類模型...\n",
      "模型訓練完成！\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"正在訓練邏輯回歸情感分類模型...\")\n",
    "\n",
    "# 檢查是否有 TF-IDF 特徵矩陣\n",
    "if 'X_train_tfidf' in locals() and 'y_train' in locals():\n",
    "    # 初始化邏輯回歸模型\n",
    "    model = LogisticRegression(random_state=42, max_iter=1000) # 增加 max_iter 以確保收斂\n",
    "    \n",
    "    # 在 TF-IDF 轉換後的訓練集上訓練模型\n",
    "    model.fit(X_train_tfidf, y_train)\n",
    "    \n",
    "    print(\"模型訓練完成！\")\n",
    "else:\n",
    "    print(\"錯誤：未找到 TF-IDF 特徵矩陣，請先執行特徵提取步驟。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f42d3e7",
   "metadata": {},
   "source": [
    "\n",
    "**討論**：\n",
    "\n",
    "邏輯回歸模型現在已經從 TF-IDF 特徵中學習到了評論文本與其情感極性之間的關係。由於邏輯回歸是一個線性模型，它將根據詞語的 TF-IDF 權重來判斷評論是正面還是負面。例如，高權重的正面詞語會增加正面情感的概率，反之亦然。接下來，我們將評估模型在未見過的測試集上的表現。\n",
    "\n",
    "## 6. 模型評估：量化情感分析的準確性\n",
    "\n",
    "在訓練完模型後，評估其在測試集上的性能至關重要。這可以讓我們了解模型在實際應用中對新評論的情感判斷能力。我們將使用以下標準分類指標：\n",
    "-   **準確率 (Accuracy Score)**：模型正確預測的樣本比例。\n",
    "-   **分類報告 (Classification Report)**：提供精確度 (Precision)、召回率 (Recall) 和 F1 分數 (F1-Score) 等更詳細的指標，針對每個類別（正面/負面）進行評估。\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "fe7386d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在評估模型性能...\n",
      "\n",
      "模型在測試集上的準確率: 0.8883\n",
      "\n",
      "分類報告：\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "    Negative       0.90      0.88      0.89      5000\n",
      "    Positive       0.88      0.90      0.89      5000\n",
      "\n",
      "    accuracy                           0.89     10000\n",
      "   macro avg       0.89      0.89      0.89     10000\n",
      "weighted avg       0.89      0.89      0.89     10000\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "\n",
    "print(\"正在評估模型性能...\")\n",
    "\n",
    "# 檢查是否有訓練好的模型和測試集\n",
    "if 'model' in locals() and 'X_test_tfidf' in locals() and 'y_test' in locals():\n",
    "    # 在測試集上進行預測\n",
    "    y_pred = model.predict(X_test_tfidf)\n",
    "    \n",
    "    # 計算準確率\n",
    "    accuracy = accuracy_score(y_test, y_pred)\n",
    "    \n",
    "    # 生成分類報告，顯示每個類別的精確度、召回率、F1分數和支持數\n",
    "    report = classification_report(y_test, y_pred, target_names=['Negative', 'Positive'])\n",
    "    \n",
    "    print(f\"\\n模型在測試集上的準確率: {accuracy:.4f}\")\n",
    "    print(\"\\n分類報告：\")\n",
    "    print(report)\n",
    "else:\n",
    "    print(\"錯誤：未找到已訓練的模型或測試集，請先執行模型訓練步驟。\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2b8333e",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "**結果解讀與討論**：\n",
    "\n",
    "模型的準確率和分類報告提供了其性能的量化評估。高準確率（接近 1）表示模型在判斷評論情感方面表現良好。分類報告則更詳細地展示了模型在識別正面和負面評論時的精確度、召回率和 F1 分數。這些指標共同表明了基於 TF-IDF 和邏輯回歸的情感分析模型，在 IMDB 資料集上能夠實現有效的文本情感判斷。\n",
    "\n",
    "## 7. 範例預測：親身體驗情感分析\n",
    "\n",
    "為了更直觀地感受模型的工作方式，我們將對一些新的、未經訓練的電影評論文本進行情感預測。這將展示模型的實際應用能力，以及如何將新的原始文本數據輸入到已訓練好的模型中獲取預測結果。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "30a993a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "正在運行範例預測...\n",
      "範例預測結果：\n",
      "評論：'This movie was absolutely fantastic! The acting was superb and the story was captivating.'\n",
      "預測情感：正面 (Positive)\n",
      "\n",
      "評論：'A complete waste of time. The plot was predictable and the characters were boring.'\n",
      "預測情感：負面 (Negative)\n",
      "\n",
      "評論：'The movie was okay, but the ending was a bit disappointing.'\n",
      "預測情感：負面 (Negative)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print(\"正在運行範例預測...\")\n",
    "\n",
    "# 檢查是否有訓練好的模型和向量化器\n",
    "if 'model' in locals() and 'tfidf_vectorizer' in locals() and 'preprocess_text' in locals():\n",
    "    # 範例評論列表\n",
    "    sample_reviews = [\n",
    "        \"This movie was absolutely fantastic! The acting was superb and the story was captivating.\",\n",
    "        \"A complete waste of time. The plot was predictable and the characters were boring.\",\n",
    "        \"The movie was okay, but the ending was a bit disappointing.\" # 新增一個中性/混合情感的評論\n",
    "    ]\n",
    "    \n",
    "    # 對範例評論進行預處理，使其符合模型輸入要求\n",
    "    cleaned_samples = [preprocess_text(review) for review in sample_reviews]\n",
    "    \n",
    "    # 使用訓練好的 TF-IDF 模型轉換範例評論為特徵向量\n",
    "    samples_tfidf = tfidf_vectorizer.transform(cleaned_samples)\n",
    "    \n",
    "    # 使用訓練好的邏輯回歸模型進行預測\n",
    "    predictions = model.predict(samples_tfidf)\n",
    "    \n",
    "    print(\"範例預測結果：\")\n",
    "    for i, review in enumerate(sample_reviews):\n",
    "        sentiment = \"正面 (Positive)\" if predictions[i] == 1 else \"負面 (Negative)\"\n",
    "        print(f\"評論：'{review}'\")\n",
    "        print(f\"預測情感：{sentiment}\\n\")\n",
    "else:\n",
    "    print(\"錯誤：未找到已訓練的模型或向量化器，請先執行完整的訓練流程。\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c0ce3d2",
   "metadata": {},
   "source": [
    "\n",
    "**討論**：\n",
    "\n",
    "範例預測展示了模型如何將新的、未見過的電影評論文本轉換為數值特徵，並成功判斷其情感極性。這印證了整個文本特徵工程和情感分析流程的有效性。即使是更複雜的、帶有諷刺或中性情感的評論，模型也能嘗試給出判斷，雖然其準確性會因文本的微妙之處而有所波動。\n",
    "\n",
    "## 8. 總結：文本特徵工程與情感分析的端到端實踐\n",
    "\n",
    "IMDB 電影評論情感分析案例是一個典型的自然語言處理任務，它完美地展示了如何將非結構化文本數據轉化為機器學習模型可理解的數值特徵，並在此基礎上構建情感分類器。這個案例整合了文本預處理、TF-IDF 特徵提取、資料分割和模型訓練評估等關鍵環節，為您提供了從原始文本到情感洞察的端到端實踐經驗。\n",
    "\n",
    "本案例的核心學習點和應用技術包括：\n",
    "\n",
    "| 步驟/技術 | 核心任務 | 關鍵考量點 |\n",
    "|:---|:---|:---|\n",
    "| **資料載入** | 從原始文本檔案讀取數據並整合 | 檔案結構、編碼、錯誤處理、NLTK 數據下載 |\n",
    "| **文本預處理** | 清理噪音，標準化文本 | 移除 HTML/標點、小寫化、分詞、停用詞移除、單詞長度過濾 |\n",
    "| **資料分割** | 劃分訓練集和測試集 | 隨機分割 (文本獨立性)，`stratify` 確保類別比例一致 |\n",
    "| **TF-IDF 特徵提取** | 將清洗後的文本轉為數值向量 | `TfidfVectorizer` 參數 (如 `max_features`, `stop_words`), 稀疏矩陣處理 |\n",
    "| **模型訓練** | 使用邏輯回歸進行情感分類 | `LogisticRegression`，`max_iter` 確保收斂 |\n",
    "| **模型評估** | 量化模型在測試集上的性能 | 準確率、分類報告 (精確度、召回率、F1 分數) |\n",
    "\n",
    "儘管基於 TF-IDF 的情感分析模型在許多情況下表現良好，但它仍然無法捕捉詞序信息和更深層次的語義上下文。在更複雜的 NLP 任務中，詞嵌入 (Word Embeddings) 和基於深度學習的語言模型 (如 BERT, GPT) 能夠提供更精細和上下文感知的文本表示，從而進一步提升 NLP 模型的性能。然而，本案例為您奠定了堅實的文本特徵工程基礎，是進一步探索高級 NLP 技術的起點。 "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
