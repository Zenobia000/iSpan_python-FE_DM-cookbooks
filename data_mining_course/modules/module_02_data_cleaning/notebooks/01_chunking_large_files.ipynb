{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "38e3c722",
   "metadata": {},
   "source": [
    "# 模組 2.1: 大檔案分塊處理 (Chunking Large Files)\n",
    "\n",
    "## 學習目標\n",
    "- 理解在處理大型資料集時可能遇到的記憶體限制問題。\n",
    "- 學習使用 Pandas `read_csv` 中的 `chunksize` 參數來分塊讀取資料。\n",
    "- 掌握對每個資料塊進行處理並匯總結果的基本模式。\n",
    "\n",
    "## 導論：為何需要分塊處理？\n",
    "\n",
    "在真實世界的資料分析場景中，我們時常會遇到比電腦 RAM 還要大的資料集（例如，數十 GB 的 CSV 檔案）。若嘗試一次性將整個檔案讀入一個 DataFrame，會導致 `MemoryError`，使分析無法進行。\n",
    "\n",
    "Pandas 提供了 `chunksize` 這個強大的參數，讓我們可以將大型檔案像處理串流一樣，一次只讀取一小部分（一個 \"chunk\"）到記憶體中，對其進行處理後，再讀取下一個部分。這種方法是處理大數據時不可或缺的基礎技能。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6eb48b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 導入必要的函式庫\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# 為了模擬大檔案，我們將使用鐵達尼號資料集，並設定一個很小的 chunksize\n",
    "path = 'data_mining_course/datasets/raw/titanic/train.csv'\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76b03d62",
   "metadata": {},
   "source": [
    "## 1. 使用 `chunksize` 進行迭代\n",
    "\n",
    "當在 `pd.read_csv()` 中設定了 `chunksize` 參數時，函數會返回一個迭代器（Iterator）。我們可以用 `for` 迴圈來遍歷這個迭代器，每次迴圈處理一個 chunk。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f78d5627",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 設定 chunksize，例如每次讀取 100 筆資料\n",
    "chunk_size = 100\n",
    "try:\n",
    "    # 創建一個迭代器\n",
    "    chunk_iterator = pd.read_csv(path, chunksize=chunk_size)\n",
    "    print(\"已創建 TextFileReader 迭代器...\")\n",
    "    print(f\"迭代器類型: {type(chunk_iterator)}\")\n",
    "\n",
    "    # 遍歷迭代器並查看每個 chunk 的資訊\n",
    "    total_rows = 0\n",
    "    for i, chunk in enumerate(chunk_iterator):\n",
    "        print(f\"--- Chunk {i+1} ---\")\n",
    "        print(f\"Chunk 的類型: {type(chunk)}\")\n",
    "        print(f\"Chunk 的維度: {chunk.shape}\")\n",
    "        total_rows += len(chunk)\n",
    "\n",
    "    print(f\"\\n處理完成！總共處理了 {total_rows} 筆資料。\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"找不到檔案: {path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ccf3a66",
   "metadata": {},
   "source": [
    "## 2. 實戰應用：分塊計算統計數據\n",
    "\n",
    "假設我們想計算鐵達尼號乘客的平均年齡，但檔案太大無法一次讀取。我們可以分塊讀取，計算每個 chunk 的年齡總和與人數，最後再將它們合併計算總平均值。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9cd8302",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 重新創建迭代器 (因為上一個迴圈已經用完了)\n",
    "    chunk_iterator = pd.read_csv(path, chunksize=chunk_size)\n",
    "\n",
    "    # 初始化變數來儲存累計值\n",
    "    total_age = 0\n",
    "    total_count = 0\n",
    "\n",
    "    # 遍歷每個 chunk\n",
    "    for chunk in chunk_iterator:\n",
    "        # 確保 'Age' 欄位沒有缺失值，然後累加\n",
    "        valid_ages = chunk['Age'].dropna()\n",
    "        total_age += valid_ages.sum()\n",
    "        total_count += valid_ages.count()\n",
    "\n",
    "    # 計算總平均年齡\n",
    "    average_age = total_age / total_count if total_count > 0 else 0\n",
    "\n",
    "    print(f\"分塊計算得到的平均年齡: {average_age:.2f}\")\n",
    "\n",
    "    # 一次性讀取並計算以進行驗證\n",
    "    full_df = pd.read_csv(path)\n",
    "    true_average_age = full_df['Age'].mean()\n",
    "    print(f\"一次性讀取計算的真實平均年齡: {true_average_age:.2f}\")\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"找不到檔案: {path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2692710",
   "metadata": {},
   "source": [
    "## 3. 實戰應用：分塊過濾資料\n",
    "\n",
    "另一個常見的應用是從大檔案中篩選出符合特定條件的資料。假設我們想找出所有票價 (`Fare`) 大於 100 的乘客。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae3520fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # 重新創建迭代器\n",
    "    chunk_iterator = pd.read_csv(path, chunksize=chunk_size)\n",
    "\n",
    "    # 創建一個空的 list 來存放符合條件的 chunks\n",
    "    high_fare_chunks = []\n",
    "\n",
    "    # 遍歷每個 chunk\n",
    "    for chunk in chunk_iterator:\n",
    "        # 過濾出票價大於 100 的部分\n",
    "        high_fare_chunk = chunk[chunk['Fare'] > 100]\n",
    "        high_fare_chunks.append(high_fare_chunk)\n",
    "\n",
    "    # 使用 pd.concat 將所有符合條件的 chunks 合併成一個新的 DataFrame\n",
    "    if high_fare_chunks:\n",
    "        high_fare_df = pd.concat(high_fare_chunks, ignore_index=True)\n",
    "        print(\"成功篩選出高票價乘客！\")\n",
    "        print(f\"共有 {len(high_fare_df)} 位乘客的票價大於 100。\")\n",
    "        display(high_fare_df.head())\n",
    "    else:\n",
    "        print(\"沒有找到符合條件的乘客。\")\n",
    "except FileNotFoundError:\n",
    "     print(f\"找不到檔案: {path}\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a601532f",
   "metadata": {},
   "source": [
    "## 總結\n",
    "\n",
    "在這個筆記本中，我們學習了如何使用 `chunksize` 來應對大檔案挑戰：\n",
    "- `read_csv` 中的 `chunksize` 參數會返回一個迭代器，讓我們可以逐塊處理資料。\n",
    "- 我們可以初始化變數，在迴圈中對每個 chunk 進行計算並累計結果。\n",
    "- 我們可以過濾每個 chunk，並將符合條件的部分收集起來，最後合併成一個新的 DataFrame。\n",
    "\n",
    "這種模式對於記憶體受限的環境或處理極大規模的資料集至關重要，是資料工程與分析中的一個核心基礎技能。"
   ]
  }
 ],
 "metadata": {},
 "nbformat": 4,
 "nbformat_minor": 5
}
